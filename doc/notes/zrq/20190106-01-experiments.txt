#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

    Resource allocation on trop03

        Stedigo     4
        Angece      4
        Edwalafia   4
        Onoza       4

        Fosauri     2
        Marpus      2
        Byflame     2

        Afoaviel    2
        Rusaldez    2

                    26 cores used, only 6 cores left

    Reduce the number of cores allocated to the each of the machines ?
    Kafka nodes, 2 cores each
    Zookeeper nodes, 1 core each.
    MirrorMaker nodes, 1 core each.

    If we are going to do that, can we compare performance before/after ?

    Ideally we want to make all of these VMs visible across the Uni network.
    Change them to use a bridge network rather than a NAT network.

    Virtual machne systems on metagrid LAN all get 10. routable addresses, and are visible to other hosts on the same network.

# -----------------------------------------------------
# Compare network config on metagrid systems.
#[user@desktop]

    source "${HOME}/ischnura.settings"

    virsh \
        --connect ${connection:?} \
            net-list \
                --all


    >    Name                 State      Autostart     Persistent
    >   ----------------------------------------------------------
    >

    We don't need to use a virtual machine network, because we control
    the main 10.x.y.z LAN network.

    The virtual machines all have MAC entries in the main DHCP service,
    which allocates them addresses on the main 10.x.y.z LAN.

    So yet again we have to waste time trying to fit our components into
    the restricted straight-jacket of the ROE network ....


    What we want to do is configure our services so that components deployed
    on one trop host are visble to components deployed on another trop host.

    Ideally, visible across the whole ROE 192.168 address space.
    Ideally, visible across the whole UoE 172.16 address space.

    Option 1

        Deploy a single DHCP service to cover all of the trop machines.

        To do that we would need to have strict control over which MAC
        addresses the DHCP service responded to.

        Difficult to ensure this doesn't get noticed and cause problems.

    Option 2

        Use libvirt to handle the DHCP allocation, but as part of a bridge
        network not a NAT network.

        According to this example it should be easy ..
        https://libvirt.org/formatnetwork.html#examplesRoute


# -----------------------------------------------------
# Update the libvirt network configuration on trop04.
#[user@desktop]

# -----------------------------------------------------
# Update the libvirt storage pools on trop04.
#[user@desktop]

# -----------------------------------------------------
# Update the set of libvirt virtual machines on trop04.
#[user@desktop]

# -----------------------------------------------------
# Create a new virtual machine image (Fedora-29, 8G mem, 8G disc)
#[user@desktop]

# -----------------------------------------------------
# Create shell script functions for adding new discs to virtual machines.
#[user@desktop]

# -----------------------------------------------------
# Create shell script functions for extending the virtual machine discs.
#[user@desktop]


