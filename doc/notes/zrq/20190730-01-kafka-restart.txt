#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

# -----------------------------------------------------
# Load our virtual machine node names.
#[user@trop03]

    source "${HOME}/nodenames.txt"

    >   Zookeepers    [Fosauri Marpus Byflame]
    >   Kafka nodes   [Stedigo Angece Edwalafia Onoza]
    >   Mirror makers [Grerat Jeralenia]


# -----------------------------------------------------
# List our virtual machines.
#[user@trop03]

    source "${HOME}/libvirt.settings"
    virsh \
        --connect ${libvirtcon:?} \
        list \
            --all

    >    Id    Name                           State
    >   ----------------------------------------------------
    >    4     Umiawyth                       running
    >    5     Fosauri                        running
    >    6     Marpus                         running
    >    7     Byflame                        running
    >    25    Stedigo                        running
    >    26    Angece                         running
    >    27    Edwalafia                      running
    >    28    Onoza                          running
    >    -     Grerat                         shut off
    >    -     Jeralenia                      shut off


# -----------------------------------------------------
# Remove the Kafka nodes.
#[user@trop03]

    source "${HOME}/libvirt.settings"

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname}]"
            virsh \
                --connect ${libvirtcon:?} \
                destroy \
                    "${vmname}"

            virsh \
                --connect ${libvirtcon:?} \
                undefine \
                    "${vmname}"
        done

    >   ---- ----
    >   Node [Stedigo]
    >   Domain Stedigo destroyed
    >   Domain Stedigo has been undefined
    >   
    >   ---- ----
    >   Node [Angece]
    >   Domain Angece destroyed
    >   Domain Angece has been undefined
    >   
    >   ---- ----
    >   Node [Edwalafia]
    >   Domain Edwalafia destroyed
    >   Domain Edwalafia has been undefined
    >   
    >   ---- ----
    >   Node [Onoza]
    >   Domain Onoza destroyed
    >   Domain Onoza has been undefined


# -----------------------------------------------------
# List our storage pools.
#[user@trop03]

    source "${HOME}/libvirt.settings"
    virsh \
        --connect ${libvirtcon:?} \
        pool-list

    >    Name                 State      Autostart
    >   -------------------------------------------
    >    base                 active     yes
    >    boot-scratch         active     no
    >    data0                active     yes
    >    data1                active     yes
    >    data2                active     yes
    >    home                 active     yes
    >    images               active     yes
    >    init                 active     yes
    >    live                 active     yes


# -----------------------------------------------------
# Stop the data pools.
#[user@trop03]

    source "${HOME}/libvirt.settings"

    for poolname in data0 data1 data2
        do
            echo "---- ----"
            echo "Pool [${poolname}]"
            virsh \
                --connect ${libvirtcon:?} \
                pool-destroy \
                    "${poolname}"
        done

    >   ---- ----
    >   Pool [data0]
    >   Pool data0 destroyed
    >   
    >   ---- ----
    >   Pool [data1]
    >   Pool data1 destroyed
    >   
    >   ---- ----
    >   Pool [data2]
    >   Pool data2 destroyed


# -----------------------------------------------------
# Clean the data discs.
#[user@trop03]

    sudo rm    /data0/libvirt/images/data0/*
    sudo rmdir /data0/libvirt/images/data0
    sudo rmdir /data0/libvirt/images

    sudo rm    /data1/libvirt/images/data1/*
    sudo rmdir /data1/libvirt/images/data1
    sudo rmdir /data1/libvirt/images

    sudo rm    /data2/libvirt/images/data2/*
    sudo rmdir /data2/libvirt/images/data2
    sudo rmdir /data2/libvirt/images


# -----------------------------------------------------
# Update the pool config.
#[user@trop03]

    source "${HOME}/libvirt.settings"

    virsh \
        --connect ${libvirtcon:?} \
        pool-edit \
            'data0'

    >       <pool type='dir'>
    >         <name>data0</name>
    >         <uuid>e69f99c0-d338-4654-a2ed-8046b35f65a1</uuid>
    >         <capacity unit='bytes'>98294312960</capacity>
    >         <allocation unit='bytes'>1703862272</allocation>
    >         <available unit='bytes'>96590450688</available>
    >         <source>
    >         </source>
    >         <target>
    >   -       <path>/data0/libvirt/images/data0</path>
    >   +       <path>/data0/libvirt/</path>
    >           <permissions>
    >             <mode>0755</mode>
    >             <owner>0</owner>
    >             <group>0</group>
    >           </permissions>
    >         </target>
    >       </pool>


    virsh \
        --connect ${libvirtcon:?} \
        pool-edit \
            'data1'

    >   -       <path>/data1/libvirt/images/data1</path>
    >   +       <path>/data1/libvirt/</path>


    virsh \
        --connect ${libvirtcon:?} \
        pool-edit \
            'data2'

    >   -       <path>/data2/libvirt/images/data2</path>
    >   +       <path>/data2/libvirt</path>


# -----------------------------------------------------
# Start the data pools.
#[user@trop03]

    source "${HOME}/libvirt.settings"

    for poolname in data0 data1 data2
        do
            echo "---- ----"
            echo "Pool [${poolname}]"
            virsh \
                --connect ${libvirtcon:?} \
                pool-start \
                    "${poolname}"
        done

    >   ---- ----
    >   Pool [data0]
    >   Pool data0 started
    >   
    >   ---- ----
    >   Pool [data1]
    >   Pool data1 started
    >   
    >   ---- ----
    >   Pool [data2]
    >   Pool data2 started


# -----------------------------------------------------
# Create our Kafka nodes.
# TODO scriptable createvm
#[user@trop03]

    createvm

    >   INFO : Node name [Stedigo]
    >   INFO : Base name [fedora-29-docker-base-20190715.qcow]
    >   INFO : Base path [/var/lib/libvirt/images/base/fedora-29-docker-base-20190715.qcow]
    >   INFO : Disc name [Stedigo.qcow]
    >   INFO : Disc size [16GiB]
    >   
    >   INFO : MAC  [06:00:AC:10:05:0A]
    >   INFO : IPv4 [172.16.5.10]
    >   INFO : IPv6 []


    createvm

    >   INFO : Node name [Angece]
    >   INFO : Base name [fedora-29-docker-base-20190715.qcow]
    >   INFO : Base path [/var/lib/libvirt/images/base/fedora-29-docker-base-20190715.qcow]
    >   INFO : Disc name [Angece.qcow]
    >   INFO : Disc size [16GiB]
    >   
    >   INFO : MAC  [06:00:AC:10:05:0B]
    >   INFO : IPv4 [172.16.5.11]
    >   INFO : IPv6 []


    createvm

    >   INFO : Node name [Edwalafia]
    >   INFO : Base name [fedora-29-docker-base-20190715.qcow]
    >   INFO : Base path [/var/lib/libvirt/images/base/fedora-29-docker-base-20190715.qcow]
    >   INFO : Disc name [Edwalafia.qcow]
    >   INFO : Disc size [16GiB]
    >   
    >   INFO : MAC  [06:00:AC:10:05:0C]
    >   INFO : IPv4 [172.16.5.12]
    >   INFO : IPv6 []


    createvm

    >   INFO : Node name [Onoza]
    >   INFO : Base name [fedora-29-docker-base-20190715.qcow]
    >   INFO : Base path [/var/lib/libvirt/images/base/fedora-29-docker-base-20190715.qcow]
    >   INFO : Disc name [Onoza.qcow]
    >   INFO : Disc size [16GiB]
    >   
    >   INFO : MAC  [06:00:AC:10:05:0D]
    >   INFO : IPv4 [172.16.5.13]
    >   INFO : IPv6 []


# -----------------------------------------------------
# Define a host lookup function.
# https://askubuntu.com/questions/627906/why-is-my-etc-hosts-file-not-queried-when-nslookup-tries-to-resolve-an-address#comment1536517_627909
# TODO Add this to a toolit script.
#[user@trop03]

    getipv4()
        {
        getent hosts "${1:?}" | cut -d ' ' -f 1
        }


#---------------------------------------------------------------------
# Update the ssh keys for each node.
# TODO Add this to a toolit script.
#[user@trop03]

    source "${HOME}/nodenames.txt"

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname:?}]"

            ssh-keygen \
                -q -R \
                    "${vmname:?}"

            ssh-keyscan \
                "${vmname:?}" \
                >> "${HOME}/.ssh/known_hosts"

            ssh-keyscan \
                -t ecdsa $(getipv4 "${vmname:?}") \
                >> "${HOME}/.ssh/known_hosts"

        done

    >   ---- ----
    >   Node [Stedigo]
    >   Host Stedigo not found in /home/dmr/.ssh/known_hosts
    >   # Stedigo:22 SSH-2.0-OpenSSH_7.9
    >   # Stedigo:22 SSH-2.0-OpenSSH_7.9
    >   # Stedigo:22 SSH-2.0-OpenSSH_7.9
    >   # 172.16.5.10:22 SSH-2.0-OpenSSH_7.9
    >   ---- ----
    >   Node [Angece]
    >   Host Angece not found in /home/dmr/.ssh/known_hosts
    >   # Angece:22 SSH-2.0-OpenSSH_7.9
    >   # Angece:22 SSH-2.0-OpenSSH_7.9
    >   # Angece:22 SSH-2.0-OpenSSH_7.9
    >   # 172.16.5.11:22 SSH-2.0-OpenSSH_7.9
    >   ---- ----
    >   Node [Edwalafia]
    >   Host Edwalafia not found in /home/dmr/.ssh/known_hosts
    >   # Edwalafia:22 SSH-2.0-OpenSSH_7.9
    >   # Edwalafia:22 SSH-2.0-OpenSSH_7.9
    >   # Edwalafia:22 SSH-2.0-OpenSSH_7.9
    >   # 172.16.5.12:22 SSH-2.0-OpenSSH_7.9
    >   ---- ----
    >   Node [Onoza]
    >   Host Onoza not found in /home/dmr/.ssh/known_hosts
    >   # Onoza:22 SSH-2.0-OpenSSH_7.9
    >   # Onoza:22 SSH-2.0-OpenSSH_7.9
    >   # Onoza:22 SSH-2.0-OpenSSH_7.9
    >   # 172.16.5.13:22 SSH-2.0-OpenSSH_7.9


# -----------------------------------------------------
# Check we can login to each node.
#[user@trop03]

    source "${HOME}/libvirt.settings"
    source "${HOME}/ssh-options"

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname:?}]"

            ssh \
                ${sshopts[*]} \
                ${sshuser:?}@${vmname:?} \
                    "
                    date
                    hostname
                    "
        done

    >   ---- ----
    >   Node [Stedigo]
    >   Tue 30 Jul 14:39:07 BST 2019
    >   Stedigo
    >   ---- ----
    >   Node [Angece]
    >   Tue 30 Jul 14:39:07 BST 2019
    >   Angece
    >   ---- ----
    >   Node [Edwalafia]
    >   Tue 30 Jul 14:39:08 BST 2019
    >   Edwalafia
    >   ---- ----
    >   Node [Onoza]
    >   Tue 30 Jul 14:39:09 BST 2019
    >   Onoza



# -----------------------------------------------------
# Create a data volume for each node.
#[user@trop03]

    source "${HOME}/libvirt.settings"

    cat > "${HOME}/nodevols" << EOF
Stedigo     data1 01  512G  vdc /data1-01
Angece      data1 01  512G  vdc /data1-01
Edwalafia   data1 01  512G  vdc /data1-01
Onoza       data1 01  512G  vdc /data1-01

Stedigo     data2 01  512G  vdd /data2-01
Angece      data2 01  512G  vdd /data2-01
Edwalafia   data2 01  512G  vdd /data2-01
Onoza       data2 01  512G  vdd /data2-01
EOF


    while read -r vmname volpool volnum volsize voldev volmnt
        do
            volname=${vmname}-${volpool}-${volnum}.qcow
            if [ "${vmname}" != '' ]
                then
                    echo "----"
                    echo "Creating [${volname}][${volsize}]"

                    virsh \
                        --connect ${libvirtcon:?} \
                        vol-create-as \
                            "${volpool}" \
                            "${volname}" \
                            "${volsize}" \
                            --allocation 0 \
                            --format qcow2

                    virsh \
                        --connect ${libvirtcon:?} \
                        vol-info \
                            --pool "${volpool:?}" \
                            "${volname:?}"

                    volpath=$(
                        virsh \
                            --connect ${libvirtcon:?} \
                            vol-path \
                                --pool "${volpool:?}" \
                                "${volname:?}"
                        )

                    echo "Adding [${volname}][${target}]"

                    virsh \
                        --connect ${libvirtcon:?} \
                        attach-disk \
                            "${vmname:?}"  \
                            "${volpath:?}" \
                            "${voldev:?}"  \
                            --subdriver qcow2 \
                            --driver qemu  \
                            --config


                fi

        done < "${HOME}/nodevols"


    >   ----
    >   Creating [Stedigo-data1-01.qcow][512G]
    >   Vol Stedigo-data1-01.qcow created
    >   
    >   Name:           Stedigo-data1-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Stedigo-data1-01.qcow][vdc]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Angece-data1-01.qcow][512G]
    >   Vol Angece-data1-01.qcow created
    >   
    >   Name:           Angece-data1-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Angece-data1-01.qcow][vdc]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Edwalafia-data1-01.qcow][512G]
    >   Vol Edwalafia-data1-01.qcow created
    >   
    >   Name:           Edwalafia-data1-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Edwalafia-data1-01.qcow][vdc]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Onoza-data1-01.qcow][512G]
    >   Vol Onoza-data1-01.qcow created
    >   
    >   Name:           Onoza-data1-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Onoza-data1-01.qcow][vdc]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Stedigo-data2-01.qcow][512G]
    >   Vol Stedigo-data2-01.qcow created
    >   
    >   Name:           Stedigo-data2-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Stedigo-data2-01.qcow][vdd]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Angece-data2-01.qcow][512G]
    >   Vol Angece-data2-01.qcow created
    >   
    >   Name:           Angece-data2-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Angece-data2-01.qcow][vdd]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Edwalafia-data2-01.qcow][512G]
    >   Vol Edwalafia-data2-01.qcow created
    >   
    >   Name:           Edwalafia-data2-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Edwalafia-data2-01.qcow][vdd]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Onoza-data2-01.qcow][512G]
    >   Vol Onoza-data2-01.qcow created
    >   
    >   Name:           Onoza-data2-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Onoza-data2-01.qcow][vdd]
    >   Disk attached successfully


# -----------------------------------------------------
# List the volumes on each Kafka node.
# http://xmlstar.sourceforge.net/doc/UG/ch04.html
# https://sourceforge.net/p/xmlstar/discussion/226076/thread/d5eca10f/#56b4
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            echo "$(printf '+ %56s +' '')" | sed 's/ /-/g'
            echo "$(printf '| %-56s |' 'Node ['${vmname:?}']')"
            virsh \
                --connect "${libvirtcon:?}" \
                dumpxml \
                    "${vmname}" \
            | xmlstarlet \
                select \
                    --text \
                    --template \
                        --match '//disk' \
                        --sort  'A:T:-' 'target/@dev' \
                        --value-of "concat('| ', target/@dev, ' | ', str:align(source/@file, str:padding(50, ' '), 'left'), ' |')" \
                        --nl
        done \
    ; echo "$(printf '+ %56s +' '')" | sed 's/ /-/g'


    >   +----------------------------------------------------------+
    >   | Node [Stedigo]                                           |
    >   | vda | /var/lib/libvirt/images/live/Stedigo.qcow          |
    >   | vdb | /var/lib/libvirt/images/init/Stedigo.iso           |
    >   +----------------------------------------------------------+
    >   | Node [Angece]                                            |
    >   | vda | /var/lib/libvirt/images/live/Angece.qcow           |
    >   | vdb | /var/lib/libvirt/images/init/Angece.iso            |
    >   +----------------------------------------------------------+
    >   | Node [Edwalafia]                                         |
    >   | vda | /var/lib/libvirt/images/live/Edwalafia.qcow        |
    >   | vdb | /var/lib/libvirt/images/init/Edwalafia.iso         |
    >   +----------------------------------------------------------+
    >   | Node [Onoza]                                             |
    >   | vda | /var/lib/libvirt/images/live/Onoza.qcow            |
    >   | vdb | /var/lib/libvirt/images/init/Onoza.iso             |
    >   +----------------------------------------------------------+


# -----------------------------------------------------
# Shutdown and restart each of our Kafka nodes.
#[user@trop03]

    source "${HOME}/libvirt.settings"

    for vmname in ${kfnames[@]}
        do
            virsh \
                --connect ${libvirtcon:?} \
                    shutdown \
                    ${vmname:?}
        done

    >   Domain Stedigo is being shutdown
    >   Domain Angece is being shutdown
    >   Domain Edwalafia is being shutdown
    >   Domain Onoza is being shutdown

    sleep 60

    for vmname in ${kfnames[@]}
        do
            virsh \
                --connect ${libvirtcon:?} \
                    start \
                    ${vmname:?}
        done

    >   Domain Stedigo started
    >   Domain Angece started
    >   Domain Edwalafia started
    >   Domain Onoza started


# -----------------------------------------------------
# List the volumes on each Kafka node.
# http://xmlstar.sourceforge.net/doc/UG/ch04.html
# https://sourceforge.net/p/xmlstar/discussion/226076/thread/d5eca10f/#56b4
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            echo "$(printf '+ %56s +' '')" | sed 's/ /-/g'
            echo "$(printf '| %-56s |' 'Node ['${vmname:?}']')"
            virsh \
                --connect "${libvirtcon:?}" \
                dumpxml \
                    "${vmname}" \
            | xmlstarlet \
                select \
                    --text \
                    --template \
                        --match '//disk' \
                        --sort  'A:T:-' 'target/@dev' \
                        --value-of "concat('| ', target/@dev, ' | ', str:align(source/@file, str:padding(50, ' '), 'left'), ' |')" \
                        --nl
        done \
    ; echo "$(printf '+ %56s +' '')" | sed 's/ /-/g'

    >   +----------------------------------------------------------+
    >   | Node [Stedigo]                                           |
    >   | vda | /var/lib/libvirt/images/live/Stedigo.qcow          |
    >   | vdb | /var/lib/libvirt/images/init/Stedigo.iso           |
    >   | vdc | /data1/libvirt/Stedigo-data1-01.qcow               |
    >   | vdd | /data2/libvirt/Stedigo-data2-01.qcow               |
    >   +----------------------------------------------------------+
    >   | Node [Angece]                                            |
    >   | vda | /var/lib/libvirt/images/live/Angece.qcow           |
    >   | vdb | /var/lib/libvirt/images/init/Angece.iso            |
    >   | vdc | /data1/libvirt/Angece-data1-01.qcow                |
    >   | vdd | /data2/libvirt/Angece-data2-01.qcow                |
    >   +----------------------------------------------------------+
    >   | Node [Edwalafia]                                         |
    >   | vda | /var/lib/libvirt/images/live/Edwalafia.qcow        |
    >   | vdb | /var/lib/libvirt/images/init/Edwalafia.iso         |
    >   | vdc | /data1/libvirt/Edwalafia-data1-01.qcow             |
    >   | vdd | /data2/libvirt/Edwalafia-data2-01.qcow             |
    >   +----------------------------------------------------------+
    >   | Node [Onoza]                                             |
    >   | vda | /var/lib/libvirt/images/live/Onoza.qcow            |
    >   | vdb | /var/lib/libvirt/images/init/Onoza.iso             |
    >   | vdc | /data1/libvirt/Onoza-data1-01.qcow                 |
    >   | vdd | /data2/libvirt/Onoza-data2-01.qcow                 |
    >   +----------------------------------------------------------+


#---------------------------------------------------------------------
# Create a script to mount a volume.
#[user@trop03]

cat > '/tmp/volume-mount.sh' << 'EOSH'

echo "---- ----"
echo "hostname [$(hostname)]"
echo "devpath  [${devpath:?}]"
echo "mntpath  [${mntpath:?}]"
echo "---- ----"

#---------------------------------------------------------------------
# Check if the new device has a filesystem.

    sudo btrfs filesystem show "${devpath:?}" > /dev/null 2>&1
    fscheck=$?

#---------------------------------------------------------------------
# Create a filesystem on the new device.

    if [ ${fscheck} == 1 ]
    then
        echo "Creating btrfs filesystem [${devpath:?}]"
        sudo \
            mkfs.btrfs \
                ${devpath:?}
    else
        echo "Found existing filesystem [${devpath:?}]"
    fi

#---------------------------------------------------------------------
# Create our mount point.

    echo "Creating mount point [${mntpath:?}]"
    sudo mkdir -p "${mntpath:?}"
    sudo touch "${mntpath:?}/mount-failed"

#---------------------------------------------------------------------
# Add the volume to our FileSystemTABle.
# https://www.howtoforge.com/reducing-disk-io-by-mounting-partitions-with-noatime

    devuuid=$(
        lsblk --noheadings --output UUID "${devpath:?}"
        )

    echo "Registering filesystem [${mntpath:?}]"
    sudo tee -a /etc/fstab << EOTAB
UUID=${devuuid:?} ${mntpath:?}    btrfs    defaults,noatime    0  0
EOTAB

#---------------------------------------------------------------------
# Mount the new volume.

    sudo \
        mount "${mntpath:?}"

#---------------------------------------------------------------------
# Check the new volume.

    echo "Checking data space [${mntpath:?}]"
    df -h "${mntpath:?}"

EOSH

# -----------------------------------------------------
# Login and mount each of the data volumes.
# https://www.linuxquestions.org/questions/linux-newbie-8/awk-special-character-as-delimiter-4175613862/
#[user@trop03]

    while read -r -u 5 vmname volpool volnum volsize voldev volmnt
        do
            if [[ ("${vmname}" != '') && ("${vmname:0:1}" != '#') ]]
                then
                    if [ ("${volmnt}" != '-') ]
                        then
                            devpath=/dev/${voldev:?}
                            mntpath=${volmnt}

                            echo ""
                            echo "---- ---- ---- ----"
                            echo "Mounting [${vmname}][${devpath}][${mntpath}]"
                            ssh ${sshopts[*]} \
                                ${sshuser:?}@${vmname:?} \
                                    "
                                    export devpath=${devpath:?}
                                    export mntpath=${mntpath:?}
                                    date
                                    hostname
                                    echo "[\${devpath}][\${mntpath}]"

                                    $(cat /tmp/volume-mount.sh)
                                    "
                        fi
                fi
        done 5< "${HOME}/nodevols"

    >   ---- ---- ---- ----
    >   Mounting [Stedigo][/dev/vdc][/data1-01]
    >   Tue 30 Jul 16:19:08 BST 2019
    >   Stedigo
    >   [/dev/vdc][/data1-01]
    >   ---- ----
    >   hostname [Stedigo]
    >   devpath  [/dev/vdc]
    >   mntpath  [/data1-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdc]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               0d45c55f-3c0c-40fe-b31b-1ef2d0c58d08
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdc
    >   
    >   Creating mount point [/data1-01]
    >   Registering filesystem [/data1-01]
    >   UUID=0d45c55f-3c0c-40fe-b31b-1ef2d0c58d08 /data1-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data1-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdc        512G   17M  510G   1% /data1-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Angece][/dev/vdc][/data1-01]
    >   Tue 30 Jul 16:31:07 BST 2019
    >   Angece
    >   [/dev/vdc][/data1-01]
    >   ---- ----
    >   hostname [Angece]
    >   devpath  [/dev/vdc]
    >   mntpath  [/data1-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdc]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               b08d6be6-7577-43ef-a187-c8257872b019
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdc
    >   
    >   Creating mount point [/data1-01]
    >   Registering filesystem [/data1-01]
    >   UUID=b08d6be6-7577-43ef-a187-c8257872b019 /data1-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data1-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdc        512G   17M  510G   1% /data1-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Edwalafia][/dev/vdc][/data1-01]
    >   Tue 30 Jul 16:31:08 BST 2019
    >   Edwalafia
    >   [/dev/vdc][/data1-01]
    >   ---- ----
    >   hostname [Edwalafia]
    >   devpath  [/dev/vdc]
    >   mntpath  [/data1-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdc]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               0971694a-a142-487f-88f6-4247d237a689
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdc
    >   
    >   Creating mount point [/data1-01]
    >   Registering filesystem [/data1-01]
    >   UUID=0971694a-a142-487f-88f6-4247d237a689 /data1-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data1-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdc        512G   17M  510G   1% /data1-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Onoza][/dev/vdc][/data1-01]
    >   Tue 30 Jul 16:31:10 BST 2019
    >   Onoza
    >   [/dev/vdc][/data1-01]
    >   ---- ----
    >   hostname [Onoza]
    >   devpath  [/dev/vdc]
    >   mntpath  [/data1-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdc]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               743c2fd0-6266-4b19-acb8-7b3c3dd6fed1
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdc
    >   
    >   Creating mount point [/data1-01]
    >   Registering filesystem [/data1-01]
    >   UUID=743c2fd0-6266-4b19-acb8-7b3c3dd6fed1 /data1-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data1-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdc        512G   17M  510G   1% /data1-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Stedigo][/dev/vdd][/data2-01]
    >   Tue 30 Jul 16:31:11 BST 2019
    >   Stedigo
    >   [/dev/vdd][/data2-01]
    >   ---- ----
    >   hostname [Stedigo]
    >   devpath  [/dev/vdd]
    >   mntpath  [/data2-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdd]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               6b1f850a-1095-4f3a-ac5a-b60510b39e2f
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdd
    >   
    >   Creating mount point [/data2-01]
    >   Registering filesystem [/data2-01]
    >   UUID=6b1f850a-1095-4f3a-ac5a-b60510b39e2f /data2-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data2-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdd        512G   17M  510G   1% /data2-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Angece][/dev/vdd][/data2-01]
    >   Tue 30 Jul 16:31:12 BST 2019
    >   Angece
    >   [/dev/vdd][/data2-01]
    >   ---- ----
    >   hostname [Angece]
    >   devpath  [/dev/vdd]
    >   mntpath  [/data2-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdd]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               2336c2f8-9038-4f08-95db-f8df73a9d102
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdd
    >   
    >   Creating mount point [/data2-01]
    >   Registering filesystem [/data2-01]
    >   UUID=2336c2f8-9038-4f08-95db-f8df73a9d102 /data2-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data2-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdd        512G   17M  510G   1% /data2-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Edwalafia][/dev/vdd][/data2-01]
    >   Tue 30 Jul 16:31:13 BST 2019
    >   Edwalafia
    >   [/dev/vdd][/data2-01]
    >   ---- ----
    >   hostname [Edwalafia]
    >   devpath  [/dev/vdd]
    >   mntpath  [/data2-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdd]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               7f13b5d9-6ae8-4426-85be-521f547f3b0b
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdd
    >   
    >   Creating mount point [/data2-01]
    >   Registering filesystem [/data2-01]
    >   UUID=7f13b5d9-6ae8-4426-85be-521f547f3b0b /data2-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data2-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdd        512G   17M  510G   1% /data2-01
    >   ----
    >   Mounting [Onoza][/dev/vdd][/data2-01]
    >   Tue 30 Jul 16:31:15 BST 2019
    >   Onoza
    >   [/dev/vdd][/data2-01]
    >   ---- ----
    >   hostname [Onoza]
    >   devpath  [/dev/vdd]
    >   mntpath  [/data2-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdd]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               97720dfb-6717-47e4-8b96-88a7bccb10e2
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdd
    >   
    >   Creating mount point [/data2-01]
    >   Registering filesystem [/data2-01]
    >   UUID=97720dfb-6717-47e4-8b96-88a7bccb10e2 /data2-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data2-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdd        512G   17M  510G   1% /data2-01


# -----------------------------------------------------
# Create our template compose file.
#[user@trop03]

cat > /tmp/kafka.yml << 'EOYML'

version: "3.2"

services:

    emily:
        image:
            "confluentinc/cp-kafka:4.1.1"
        restart:
            "no"
        ports:
            - "9092:9092"
            - "9093:9093"
        extra_hosts:
            - "${KAFKA_HOSTNAME}:127.0.0.2"
        environment:
            - "KAFKA_LISTENERS=jasminum://0.0.0.0:9092"
            - "KAFKA_ADVERTISED_LISTENERS=jasminum://${KAFKA_HOSTNAME}:9092"
            - "KAFKA_INTER_BROKER_LISTENER_NAME=jasminum"
            - "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=jasminum:PLAINTEXT"
            - "KAFKA_LOG_DIRS=${KAFKA_LOG_DIRS}"
            - "KAFKA_BROKER_ID=${KAFKA_BROKER_ID}"
            - "KAFKA_BROKER_RACK=${KAFKA_BROKER_RACK}"
            - "KAFKA_ZOOKEEPER_CONNECT=${KAFKA_ZOOKEEPER_CONNECT}"
            - "KAFKA_NUM_PARTITIONS=16"
            - "KAFKA_DEFAULT_REPLICATION_FACTOR=3"
            - "KAFKA_LOG_RETENTION_MS=-1"
            - "KAFKA_LOG_RETENTION_BYTES=-1"
            - "KAFKA_AUTO_CREATE_TOPICS_ENABLE=true"
            - "KAFKA_MESSAGE_MAX_BYTES=10485760"
        volumes:
EOYML


# -----------------------------------------------------
# Create a compose file for each node.
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            cp /tmp/kafka.yml /tmp/${vmname}-kafka.yml
        done


# -----------------------------------------------------
# Add the list of volumes to our compose files.
#[user@trop03]

        while read -r -u 5 vmname volpool volnum volsize voldev volmnt
            do
                if [[ ("${vmname}" != '') && ("${vmname:0:1}" != '#') ]]
                    then
                        if [[ ("${volmnt}" != '-') ]]
                            then
cat >> /tmp/${vmname}-kafka.yml << EOF
            - type:   "bind"
              source: "${volmnt}"
              target: "${volmnt}"
EOF
                            fi
                    fi
            done 5< "${HOME}/nodevols"


# -----------------------------------------------------
# Deploy our compose files.
#[user@trop03]

    source "${HOME}/ssh-options"

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname}]"
            scp \
                ${scpopts[*]} \
                /tmp/${vmname:?}-kafka.yml \
                ${sshuser:?}@${vmname:?}:kafka.yml
        done

    >   ---- ----
    >   Node [Stedigo]
    >   Stedigo-kafka.yml       100% 1259     1.4MB/s   00:00
    >   ---- ----
    >   Node [Angece]
    >   Angece-kafka.yml        100% 1259     1.4MB/s   00:00
    >   ---- ----
    >   Node [Edwalafia]
    >   Edwalafia-kafka.yml     100% 1259     1.5MB/s   00:00
    >   ---- ----
    >   Node [Onoza]
    >   Onoza-kafka.yml         100% 1259     1.4MB/s   00:00


# -----------------------------------------------------
# Make a list of our Zookeeper nodes.
#[user@trop03]

    zklist=${zknames[*]}
    zklist=${zklist// /,}

    echo "zklist [${zklist}]"

    >   zklist [Fosauri,Marpus,Byflame]


# -----------------------------------------------------
# Deploy a compose ENV file to each node.
#[user@trop03]

    unset logdirs
    declare logdirs=(
        '/data1-01',
        '/data2-01',
        '/data1-01',
        '/data2-01'
        )

    for (( i=0 ; i < ${#kfnames[@]} ; i++ ))
        do
            vmname=${kfnames[$i]:?}
            logdir=${logdirs[$i]:?}

            echo "---- ----"
            echo "Node [${i:?}][${vmname:?}]"

            ssh \
                ${scpopts[*]} \
                ${sshuser:?}@${vmname:?} \
                "
                hostname
                date

cat > kafka.env << EOF
KAFKA_LOG_DIRS=${logdir:?}
KAFKA_BROKER_ID=$(($i+1))
KAFKA_BROKER_RACK=$(($i+1))
KAFKA_ZOOKEEPER_CONNECT=${zklist:?}
KAFKA_HOSTNAME=${vmname:?}
EOF

                ln -sf kafka.env .env
                "
        done

    >   ---- ----
    >   Node [0][Stedigo]
    >   Stedigo
    >   Wed 31 Jul 02:20:05 BST 2019
    >   ---- ----
    >   Node [1][Angece]
    >   Angece
    >   Wed 31 Jul 02:20:05 BST 2019
    >   ---- ----
    >   Node [2][Edwalafia]
    >   Edwalafia
    >   Wed 31 Jul 02:20:06 BST 2019
    >   ---- ----
    >   Node [3][Onoza]
    >   Onoza
    >   Wed 31 Jul 02:20:06 BST 2019


# -----------------------------------------------------
# Check the compose ENV file on each node.
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname}]"
            ssh \
                ${sshopts[*]} \
                ${sshuser:?}@${vmname:?} \
                    "
                    hostname
                    date
                    cat .env
                    "
        done

    >   ---- ----
    >   Node [Stedigo]
    >   Stedigo
    >   Wed 31 Jul 02:20:36 BST 2019
    >   KAFKA_LOG_DIRS=/data1-01,
    >   KAFKA_BROKER_ID=1
    >   KAFKA_BROKER_RACK=1
    >   KAFKA_ZOOKEEPER_CONNECT=Fosauri,Marpus,Byflame
    >   KAFKA_HOSTNAME=Stedigo
    >   ---- ----
    >   Node [Angece]
    >   Angece
    >   Wed 31 Jul 02:20:37 BST 2019
    >   KAFKA_LOG_DIRS=/data2-01,
    >   KAFKA_BROKER_ID=2
    >   KAFKA_BROKER_RACK=2
    >   KAFKA_ZOOKEEPER_CONNECT=Fosauri,Marpus,Byflame
    >   KAFKA_HOSTNAME=Angece
    >   ---- ----
    >   Node [Edwalafia]
    >   Edwalafia
    >   Wed 31 Jul 02:20:37 BST 2019
    >   KAFKA_LOG_DIRS=/data1-01,
    >   KAFKA_BROKER_ID=3
    >   KAFKA_BROKER_RACK=3
    >   KAFKA_ZOOKEEPER_CONNECT=Fosauri,Marpus,Byflame
    >   KAFKA_HOSTNAME=Edwalafia
    >   ---- ----
    >   Node [Onoza]
    >   Onoza
    >   Wed 31 Jul 02:20:38 BST 2019
    >   KAFKA_LOG_DIRS=/data2-01
    >   KAFKA_BROKER_ID=4
    >   KAFKA_BROKER_RACK=4
    >   KAFKA_ZOOKEEPER_CONNECT=Fosauri,Marpus,Byflame
    >   KAFKA_HOSTNAME=Onoza


# -----------------------------------------------------
# Start Kafka on each node.
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname}]"
            ssh \
                ${scpopts[*]} \
                ${sshuser:?}@${vmname:?} \
                "
                hostname
                date
                docker-compose \
                    --file kafka.yml \
                    up -d
                "
        done




# -----------------------------------------------------
# -----------------------------------------------------
# Tail the logs on each node.
# https://www.systutorials.com/docs/linux/man/1-gnome-terminal/
# https://www.systutorials.com/docs/linux/man/7-X/#lbAH
#[user@desktop]

    mate-terminal \
        --geometry '160x10+25+25' \
        --command '
            ssh -t Stedigo "
                date
                hostname
                docker logs -f stevedore_emily_1
                \${SHELL}
                "
            '
    sleep 1

    mate-terminal \
        --geometry '160x10+125+125' \
        --command '
            ssh -t Angece "
                date
                hostname
                docker logs -f stevedore_emily_1
                \${SHELL}
                "
            '
    sleep 1

    mate-terminal \
        --geometry '160x10+225+225' \
        --command '
            ssh -t Edwalafia "
                date
                hostname
                docker logs -f stevedore_emily_1
                \${SHELL}
                "
            '
    sleep 1

    mate-terminal \
        --geometry '160x10+325+325' \
        --command '
            ssh -t Onoza "
                date
                hostname
                docker logs -f stevedore_emily_1
                \${SHELL}
                "
            '



























