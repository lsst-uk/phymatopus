#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2018, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


    Spark Streaming Pushing the Throughout Limits, the Reactive Way
    https://www.youtube.com/watch?v=qxsOjJnwcKQ&list=PL-x35fyliRwif48cPXQ1nFM85_7e200Jp


        TIL Bloom filter
        https://en.wikipedia.org/wiki/Bloom_filter
        https://www.baeldung.com/guava-bloom-filter
        https://github.com/google/guava/blob/master/guava/src/com/google/common/hash/BloomFilter.java

        Complex form - explicitly configure the Spark partitioning (+configurable, -configurable)
        Simpler form - map 1:1 Kafka:Spark partitions (+simple, -simple)

        KafkaUtils.createDirectStream[
            [key class],
            [value class],
            [key decoder class],
            [value decoder class]
            ](
                streaming context,
                [map of Kafka params],
                [set of topics]
            )


        SparkStreaming UI - very useful for development

        SparkStreaming jobs crash if data rate is too high !?

        SparkNotebook Kafka Consumer


    Spark Streaming notes
    https://github.com/jaceklaskowski/spark-streaming-notebook/blob/master/book-intro.adoc

        ....

    Spark Streaming + Kafka Integration Guide
    https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html

        Linking
        Creating a Direct Stream
        LocationStrategies
            "If your executors are on the same hosts as your Kafka brokers,
             use PreferBrokers, which will prefer to schedule partitions on
             the Kafka leader for that partition"
        ConsumerStrategies
        Creating an RDD
        Obtaining Offsets
        Storing Offsets
        SSL/TLS
        Deploying
            "As with any Spark applications, spark-submit is used to launch your application."


    Databricks
    https://en.wikipedia.org/wiki/Databricks

        "Databricks is a company founded by the creators of Apache Spark, that aims to help
         clients with cloud-based big data processing using Spark."
        "Databricks develops a web-based platform for working with Spark, that provides
         automated cluster management and IPython-style notebooks."


    Structured streaming
    https://databricks.com/glossary/what-is-structured-streaming

        "Structured Streaming is a high-level API for stream processing .. in Spark 2.2."
        "Structured Streaming allows you to take the same operations that you perform in
         batch mode using Spark’s structured APIs, and run them in a streaming fashion"


    How to Set up Kafka on Databricks
    https://docs.databricks.com/user-guide/faq/kafka-setup.html

        ....

    Real-Time End-to-End Integration with Apache Kafka in Apache Spark’s Structured Streaming
    https://databricks.com/blog/2017/04/04/real-time-end-to-end-integration-with-apache-kafka-in-apache-sparks-structured-streaming.html

        Kafka connector (read)
        Streaming ETL
        Windowing
        File output (parquet)
        JDBC connector (write)
        Kafka connector (write)


    https://en.wikipedia.org/wiki/Extract,_transform,_load
    ETL (extract, transform, load)

        "A process in database usage to prepare data for analysis"

    Getting Started with Spark Streaming, Python, and Kafka
    https://www.rittmanmead.com/blog/2017/01/getting-started-with-spark-streaming-with-python-and-kafka/

        all-spark-notebook Docker image to provide both Jupyter and the Spark runtime environment
        JupyterNotebook+ Pyhton + Spark

        Preparing the Environment
        Message Processing
        Start the streaming context
        Windowed Stream Processing

    Spark - Streaming programming guide
    https://spark.apache.org/docs/latest/streaming-programming-guide.html

        A Quick Example
        Basic Concepts
        Linking
        Initializing StreamingContext
        Discretized Streams (DStreams)
            "represents a continuous stream of data"
        Input DStreams and Receivers
        Basic Sources
            File streams
        Advanced Sources
            Kakfa
            Kinesis
            Flume
        Transformations on DStreams
        Output Operations on DStreams
        Design Patterns for using foreachRDD
        DataFrame and SQL Operations
        MLlib Operations
        Caching / Persistence
        Checkpointing
        Accumulators, Broadcast Variables, and Checkpoints
        Deploying Applications
        Monitoring Applications
        Performance Tuning
        Fault-tolerance Semantics
        Where to Go from Here


    Spark MLlib - Machine Learning Library
    https://spark.apache.org/docs/latest/ml-guide.html

        ....

    Spark GraphX -
    https://spark.apache.org/docs/latest/graphx-programming-guide.html

        Spark GraphX is a new component for graphs and graph-parallel computation.


    Simple Spark Streaming & Kafka Example in a Zeppelin Notebook
    https://henning.kropponline.de/2016/12/25/simple-spark-streaming-kafka-example-in-a-zeppelin-notebook/

        Code given in in png images that don't load  :-(

    The world beyond batch, streaming 101.
    https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101

        ....

    The world beyond batch, streaming 102.
    https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102

        ....

    Data architectures for streaming applications
    https://www.oreilly.com/ideas/data-architectures-for-streaming-applications

        ....

    Frances Perry on Apache Beam
    https://www.se-radio.net/2016/10/se-radio-episode-272-frances-perry-on-apache-beam/

            ....

    Event sourcing, CQRS, stream processing and Apache Kafka: What’s the connection?
    https://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/

    Event sourcing
    https://martinfowler.com/eaaDev/EventSourcing.html

        The fundamental idea of Event Sourcing is that of ensuring every change to the state
        of an application is captured in an event object, and that these event objects are
        themselves stored in the sequence they were applied for the same lifetime as the
        application state itself.

        .. log of state changes

    Command query responsibility segregation (CQRS)
    https://en.wikipedia.org/wiki/Command%E2%80%93query_separation
    https://martinfowler.com/bliki/CQRS.html

        "... every method should either be a command that performs an action,
        or a query that returns data to the caller, but not both.
        In other words, Asking a question should not change the answer."

    Materialized view
    https://en.wikipedia.org/wiki/Materialized_view

        "... a materialized view is a database object that contains the results of a query."


    Spark Cassandra connector
    https://github.com/datastax/spark-cassandra-connector


    Data Analytics using Cassandra and Spark
    https://opencredo.com/blogs/data-analytics-using-cassandra-and-spark/

            (Cassandra) .. is a great datastore with nice scalability and performance characteristics.

            However, adopting Cassandra as a single, one size fits all database has several downsides.
            The partitioned/distributed data storage model makes it difficult (and often very inefficient)
            to do certain types of queries or data analytics that are much more straightforward in a
            relational database.

        Spark from ten thousand feet

            DataStax ships their solution (DataStax Enterprise) with a pre-packaged integrated Cassandra
            and Spark cluster.

            Spark is the spiritual successor of Hadoop MapReduce – it’s younger, faster, nicer to use,
            and generally just better, but solves the same problem in a similar fashion.

            Spark is agnostic to the underlying storage layer – you can use HDFS, the local filesystem,
            an RDBMS, or in our case, Cassandra.

        How does it work?

            The fundamental idea is quite simple: Spark and Cassandra clusters are deployed to the same
            set of machines. Cassandra stores the data; Spark worker nodes are co-located with Cassandra
            and do the data processing.

        ...
        ...
        ...
        ...

        Conclusion

            With this post I have aimed to show some of the ways you can combine Cassandra and Spark to
            address some common challenges, which, if you were using Cassandra alone, would more than
            likely prove very problematic.

            Cassandra and Spark are technologies that fit together extremely well, both having a similar
            abstract notion of how data should be distributed and replicated across a set of computers.

            Cassandra lacks advanced querying and data processing capabilities, while Spark on its own
            does not have a persistent data store. While the examples in this post are deliberately
            simple, adding Spark into the mix allows for much more complex processing.
