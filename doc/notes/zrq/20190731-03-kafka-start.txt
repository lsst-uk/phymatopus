#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2019, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

# -----------------------------------------------------
# Load our node names.
#[user@trop03]

    source "${HOME}/nodenames"

    >   Zookeepers    [Fosauri Marpus Byflame]
    >   Kafka nodes   [Stedigo Angece Edwalafia Onoza]
    >   Mirror makers [Grerat Jeralenia]


# -----------------------------------------------------
# Create our Kafka nodes.
# TODO scriptable createvm
#[user@trop03]

    createvm

    >   INFO : Node name [Stedigo]
    >   INFO : Base name [fedora-29-docker-base-20190715.qcow]
    >   INFO : Base path [/var/lib/libvirt/images/base/fedora-29-docker-base-20190715.qcow]
    >   INFO : Disc name [Stedigo.qcow]
    >   INFO : Disc size [16GiB]
    >   
    >   INFO : MAC  [06:00:AC:10:05:0A]
    >   INFO : IPv4 [172.16.5.10]
    >   INFO : IPv6 []


    createvm

    >   INFO : Node name [Angece]
    >   INFO : Base name [fedora-29-docker-base-20190715.qcow]
    >   INFO : Base path [/var/lib/libvirt/images/base/fedora-29-docker-base-20190715.qcow]
    >   INFO : Disc name [Angece.qcow]
    >   INFO : Disc size [16GiB]
    >   
    >   INFO : MAC  [06:00:AC:10:05:0B]
    >   INFO : IPv4 [172.16.5.11]
    >   INFO : IPv6 []


    createvm

    >   INFO : Node name [Edwalafia]
    >   INFO : Base name [fedora-29-docker-base-20190715.qcow]
    >   INFO : Base path [/var/lib/libvirt/images/base/fedora-29-docker-base-20190715.qcow]
    >   INFO : Disc name [Edwalafia.qcow]
    >   INFO : Disc size [16GiB]
    >   
    >   INFO : MAC  [06:00:AC:10:05:0C]
    >   INFO : IPv4 [172.16.5.12]
    >   INFO : IPv6 []


    createvm

    >   INFO : Node name [Onoza]
    >   INFO : Base name [fedora-29-docker-base-20190715.qcow]
    >   INFO : Base path [/var/lib/libvirt/images/base/fedora-29-docker-base-20190715.qcow]
    >   INFO : Disc name [Onoza.qcow]
    >   INFO : Disc size [16GiB]
    >   
    >   INFO : MAC  [06:00:AC:10:05:0D]
    >   INFO : IPv4 [172.16.5.13]
    >   INFO : IPv6 []


# -----------------------------------------------------
# Define a host lookup function.
# https://askubuntu.com/questions/627906/why-is-my-etc-hosts-file-not-queried-when-nslookup-tries-to-resolve-an-address#comment1536517_627909
#[user@trop03]

    getipv4()
        {
        getent hosts "${1:?}" | cut -d ' ' -f 1
        }

#---------------------------------------------------------------------
# Update the ssh keys for each node.
# TODO Add this to a toolit script.
#[user@trop03]

    source "${HOME}/nodenames"

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname:?}]"

            ssh-keygen \
                -q -R \
                    "${vmname:?}"

            ssh-keyscan \
                "${vmname:?}" \
                >> "${HOME}/.ssh/known_hosts"

            ssh-keyscan \
                -t ecdsa $(getipv4 "${vmname:?}") \
                >> "${HOME}/.ssh/known_hosts"

        done

    >   ---- ----
    >   Node [Stedigo]
    >   Host Stedigo not found in /home/dmr/.ssh/known_hosts
    >   # Stedigo:22 SSH-2.0-OpenSSH_7.9
    >   # Stedigo:22 SSH-2.0-OpenSSH_7.9
    >   # Stedigo:22 SSH-2.0-OpenSSH_7.9
    >   # 172.16.5.10:22 SSH-2.0-OpenSSH_7.9
    >   ---- ----
    >   Node [Angece]
    >   Host Angece not found in /home/dmr/.ssh/known_hosts
    >   # Angece:22 SSH-2.0-OpenSSH_7.9
    >   # Angece:22 SSH-2.0-OpenSSH_7.9
    >   # Angece:22 SSH-2.0-OpenSSH_7.9
    >   # 172.16.5.11:22 SSH-2.0-OpenSSH_7.9
    >   ---- ----
    >   Node [Edwalafia]
    >   Host Edwalafia not found in /home/dmr/.ssh/known_hosts
    >   # Edwalafia:22 SSH-2.0-OpenSSH_7.9
    >   # Edwalafia:22 SSH-2.0-OpenSSH_7.9
    >   # Edwalafia:22 SSH-2.0-OpenSSH_7.9
    >   # 172.16.5.12:22 SSH-2.0-OpenSSH_7.9
    >   ---- ----
    >   Node [Onoza]
    >   Host Onoza not found in /home/dmr/.ssh/known_hosts
    >   # Onoza:22 SSH-2.0-OpenSSH_7.9
    >   # Onoza:22 SSH-2.0-OpenSSH_7.9
    >   # Onoza:22 SSH-2.0-OpenSSH_7.9
    >   # 172.16.5.13:22 SSH-2.0-OpenSSH_7.9


# -----------------------------------------------------
# Check we can login to each node.
#[user@trop03]

    source "${HOME}/libvirt.settings"
    source "${HOME}/ssh-options"

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname:?}]"

            ssh \
                ${sshopts[*]} \
                ${sshuser:?}@${vmname:?} \
                    "
                    date
                    hostname
                    "
        done

    >   ---- ----
    >   Node [Stedigo]
    >   Wed 31 Jul 06:01:59 BST 2019
    >   Stedigo
    >   ---- ----
    >   Node [Angece]
    >   Wed 31 Jul 06:02:00 BST 2019
    >   Angece
    >   ---- ----
    >   Node [Edwalafia]
    >   Wed 31 Jul 06:02:01 BST 2019
    >   Edwalafia
    >   ---- ----
    >   Node [Onoza]
    >   Wed 31 Jul 06:02:01 BST 2019
    >   Onoza


# -----------------------------------------------------
# Create a data volume for each node.
#[user@trop03]

    source "${HOME}/libvirt.settings"

    cat > "${HOME}/nodevols" << EOF
Stedigo     data1 01  512G  vdc /data1-01
Angece      data1 01  512G  vdc /data1-01
Edwalafia   data1 01  512G  vdc /data1-01
Onoza       data1 01  512G  vdc /data1-01

Stedigo     data2 01  512G  vdd /data2-01
Angece      data2 01  512G  vdd /data2-01
Edwalafia   data2 01  512G  vdd /data2-01
Onoza       data2 01  512G  vdd /data2-01
EOF


    while read -r vmname volpool volnum volsize voldev volmnt
        do
            volname=${vmname}-${volpool}-${volnum}.qcow
            if [ "${vmname}" != '' ]
                then
                    echo "----"
                    echo "Creating [${volname}][${volsize}]"

                    virsh \
                        --connect ${libvirtcon:?} \
                        vol-create-as \
                            "${volpool}" \
                            "${volname}" \
                            "${volsize}" \
                            --allocation 0 \
                            --format qcow2

                    virsh \
                        --connect ${libvirtcon:?} \
                        vol-info \
                            --pool "${volpool:?}" \
                            "${volname:?}"

                    volpath=$(
                        virsh \
                            --connect ${libvirtcon:?} \
                            vol-path \
                                --pool "${volpool:?}" \
                                "${volname:?}"
                        )

                    echo "Adding [${volname}][${target}]"

                    virsh \
                        --connect ${libvirtcon:?} \
                        attach-disk \
                            "${vmname:?}"  \
                            "${volpath:?}" \
                            "${voldev:?}"  \
                            --subdriver qcow2 \
                            --driver qemu  \
                            --config


                fi

        done < "${HOME}/nodevols"

    >   ----
    >   Creating [Stedigo-data1-01.qcow][512G]
    >   Vol Stedigo-data1-01.qcow created
    >   
    >   Name:           Stedigo-data1-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Stedigo-data1-01.qcow][]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Angece-data1-01.qcow][512G]
    >   Vol Angece-data1-01.qcow created
    >   
    >   Name:           Angece-data1-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Angece-data1-01.qcow][]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Edwalafia-data1-01.qcow][512G]
    >   Vol Edwalafia-data1-01.qcow created
    >   
    >   Name:           Edwalafia-data1-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Edwalafia-data1-01.qcow][]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Onoza-data1-01.qcow][512G]
    >   Vol Onoza-data1-01.qcow created
    >   
    >   Name:           Onoza-data1-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Onoza-data1-01.qcow][]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Stedigo-data2-01.qcow][512G]
    >   Vol Stedigo-data2-01.qcow created
    >   
    >   Name:           Stedigo-data2-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Stedigo-data2-01.qcow][]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Angece-data2-01.qcow][512G]
    >   Vol Angece-data2-01.qcow created
    >   
    >   Name:           Angece-data2-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Angece-data2-01.qcow][]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Edwalafia-data2-01.qcow][512G]
    >   Vol Edwalafia-data2-01.qcow created
    >   
    >   Name:           Edwalafia-data2-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Edwalafia-data2-01.qcow][]
    >   Disk attached successfully
    >   
    >   ----
    >   Creating [Onoza-data2-01.qcow][512G]
    >   Vol Onoza-data2-01.qcow created
    >   
    >   Name:           Onoza-data2-01.qcow
    >   Type:           file
    >   Capacity:       512.00 GiB
    >   Allocation:     200.00 KiB
    >   
    >   Adding [Onoza-data2-01.qcow][]
    >   Disk attached successfully


# -----------------------------------------------------
# List the volumes on each Kafka node.
# http://xmlstar.sourceforge.net/doc/UG/ch04.html
# https://sourceforge.net/p/xmlstar/discussion/226076/thread/d5eca10f/#56b4
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            echo "$(printf '+ %56s +' '')" | sed 's/ /-/g'
            echo "$(printf '| %-56s |' 'Node ['${vmname:?}']')"
            virsh \
                --connect "${libvirtcon:?}" \
                dumpxml \
                    "${vmname}" \
            | xmlstarlet \
                select \
                    --text \
                    --template \
                        --match '//disk' \
                        --sort  'A:T:-' 'target/@dev' \
                        --value-of "concat('| ', target/@dev, ' | ', str:align(source/@file, str:padding(50, ' '), 'left'), ' |')" \
                        --nl
        done \
    ; echo "$(printf '+ %56s +' '')" | sed 's/ /-/g'

    >   +----------------------------------------------------------+
    >   | Node [Stedigo]                                           |
    >   | vda | /var/lib/libvirt/images/live/Stedigo.qcow          |
    >   | vdb | /var/lib/libvirt/images/init/Stedigo.iso           |
    >   +----------------------------------------------------------+
    >   | Node [Angece]                                            |
    >   | vda | /var/lib/libvirt/images/live/Angece.qcow           |
    >   | vdb | /var/lib/libvirt/images/init/Angece.iso            |
    >   +----------------------------------------------------------+
    >   | Node [Edwalafia]                                         |
    >   | vda | /var/lib/libvirt/images/live/Edwalafia.qcow        |
    >   | vdb | /var/lib/libvirt/images/init/Edwalafia.iso         |
    >   +----------------------------------------------------------+
    >   | Node [Onoza]                                             |
    >   | vda | /var/lib/libvirt/images/live/Onoza.qcow            |
    >   | vdb | /var/lib/libvirt/images/init/Onoza.iso             |
    >   +----------------------------------------------------------+


# -----------------------------------------------------
# Shutdown and restart each of our Kafka nodes.
#[user@trop03]

    source "${HOME}/libvirt.settings"

    for vmname in ${kfnames[@]}
        do
            virsh \
                --connect ${libvirtcon:?} \
                    shutdown \
                    ${vmname:?}
        done

    >   Domain Stedigo is being shutdown
    >   Domain Angece is being shutdown
    >   Domain Edwalafia is being shutdown
    >   Domain Onoza is being shutdown

    sleep 60

    for vmname in ${kfnames[@]}
        do
            virsh \
                --connect ${libvirtcon:?} \
                    start \
                    ${vmname:?}
        done

    >   Domain Stedigo started
    >   Domain Angece started
    >   Domain Edwalafia started
    >   Domain Onoza started


# -----------------------------------------------------
# List the volumes on each Kafka node.
# http://xmlstar.sourceforge.net/doc/UG/ch04.html
# https://sourceforge.net/p/xmlstar/discussion/226076/thread/d5eca10f/#56b4
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            echo "$(printf '+ %56s +' '')" | sed 's/ /-/g'
            echo "$(printf '| %-56s |' 'Node ['${vmname:?}']')"
            virsh \
                --connect "${libvirtcon:?}" \
                dumpxml \
                    "${vmname}" \
            | xmlstarlet \
                select \
                    --text \
                    --template \
                        --match '//disk' \
                        --sort  'A:T:-' 'target/@dev' \
                        --value-of "concat('| ', target/@dev, ' | ', str:align(source/@file, str:padding(50, ' '), 'left'), ' |')" \
                        --nl
        done \
    ; echo "$(printf '+ %56s +' '')" | sed 's/ /-/g'

    >   +----------------------------------------------------------+
    >   | Node [Stedigo]                                           |
    >   | vda | /var/lib/libvirt/images/live/Stedigo.qcow          |
    >   | vdb | /var/lib/libvirt/images/init/Stedigo.iso           |
    >   | vdc | /data1/libvirt/Stedigo-data1-01.qcow               |
    >   | vdd | /data2/libvirt/Stedigo-data2-01.qcow               |
    >   +----------------------------------------------------------+
    >   | Node [Angece]                                            |
    >   | vda | /var/lib/libvirt/images/live/Angece.qcow           |
    >   | vdb | /var/lib/libvirt/images/init/Angece.iso            |
    >   | vdc | /data1/libvirt/Angece-data1-01.qcow                |
    >   | vdd | /data2/libvirt/Angece-data2-01.qcow                |
    >   +----------------------------------------------------------+
    >   | Node [Edwalafia]                                         |
    >   | vda | /var/lib/libvirt/images/live/Edwalafia.qcow        |
    >   | vdb | /var/lib/libvirt/images/init/Edwalafia.iso         |
    >   | vdc | /data1/libvirt/Edwalafia-data1-01.qcow             |
    >   | vdd | /data2/libvirt/Edwalafia-data2-01.qcow             |
    >   +----------------------------------------------------------+
    >   | Node [Onoza]                                             |
    >   | vda | /var/lib/libvirt/images/live/Onoza.qcow            |
    >   | vdb | /var/lib/libvirt/images/init/Onoza.iso             |
    >   | vdc | /data1/libvirt/Onoza-data1-01.qcow                 |
    >   | vdd | /data2/libvirt/Onoza-data2-01.qcow                 |
    >   +----------------------------------------------------------+


#---------------------------------------------------------------------
# Create a script to mount a volume.
#[user@trop03]

cat > '/tmp/volume-mount.sh' << 'EOSH'

echo "---- ----"
echo "hostname [$(hostname)]"
echo "devpath  [${devpath:?}]"
echo "mntpath  [${mntpath:?}]"
echo "---- ----"

#---------------------------------------------------------------------
# Check if the new device has a filesystem.

    sudo btrfs filesystem show "${devpath:?}" > /dev/null 2>&1
    fscheck=$?

#---------------------------------------------------------------------
# Create a filesystem on the new device.

    if [ ${fscheck} == 1 ]
    then
        echo "Creating btrfs filesystem [${devpath:?}]"
        sudo \
            mkfs.btrfs \
                ${devpath:?}
    else
        echo "Found existing filesystem [${devpath:?}]"
    fi

#---------------------------------------------------------------------
# Create our mount point.

    echo "Creating mount point [${mntpath:?}]"
    sudo mkdir -p "${mntpath:?}"
    sudo touch "${mntpath:?}/mount-failed"

#---------------------------------------------------------------------
# Add the volume to our FileSystemTABle.
# https://www.howtoforge.com/reducing-disk-io-by-mounting-partitions-with-noatime

    devuuid=$(
        lsblk --noheadings --output UUID "${devpath:?}"
        )

    echo "Registering filesystem [${mntpath:?}]"
    sudo tee -a /etc/fstab << EOTAB
UUID=${devuuid:?} ${mntpath:?}    btrfs    defaults,noatime    0  0
EOTAB

#---------------------------------------------------------------------
# Mount the new volume.

    sudo \
        mount "${mntpath:?}"

#---------------------------------------------------------------------
# Check the new volume.

    echo "Checking data space [${mntpath:?}]"
    df -h "${mntpath:?}"

EOSH

# -----------------------------------------------------
# Login and mount each of the data volumes.
# https://www.linuxquestions.org/questions/linux-newbie-8/awk-special-character-as-delimiter-4175613862/
#[user@trop03]

    while read -r -u 5 vmname volpool volnum volsize voldev volmnt
        do
            if [[ ("${vmname}" != '') && ("${vmname:0:1}" != '#') ]]
                then
                    if [[ ("${volmnt}" != '-') ]]
                        then
                            devpath=/dev/${voldev:?}
                            mntpath=${volmnt}

                            echo ""
                            echo "---- ---- ---- ----"
                            echo "Mounting [${vmname}][${devpath}][${mntpath}]"
                            ssh ${sshopts[*]} \
                                ${sshuser:?}@${vmname:?} \
                                    "
                                    export devpath=${devpath:?}
                                    export mntpath=${mntpath:?}
                                    date
                                    hostname
                                    echo "[\${devpath}][\${mntpath}]"

                                    $(cat /tmp/volume-mount.sh)
                                    "
                        fi
                fi
        done 5< "${HOME}/nodevols"

    >   ---- ---- ---- ----
    >   Mounting [Stedigo][/dev/vdc][/data1-01]
    >   Wed 31 Jul 06:04:57 BST 2019
    >   Stedigo
    >   [/dev/vdc][/data1-01]
    >   ---- ----
    >   hostname [Stedigo]
    >   devpath  [/dev/vdc]
    >   mntpath  [/data1-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdc]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               3128e38f-3a30-40a3-8cd9-0a7f1ff78f1f
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdc
    >   
    >   Creating mount point [/data1-01]
    >   Registering filesystem [/data1-01]
    >   UUID=3128e38f-3a30-40a3-8cd9-0a7f1ff78f1f /data1-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data1-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdc        512G   17M  510G   1% /data1-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Angece][/dev/vdc][/data1-01]
    >   Wed 31 Jul 06:04:59 BST 2019
    >   Angece
    >   [/dev/vdc][/data1-01]
    >   ---- ----
    >   hostname [Angece]
    >   devpath  [/dev/vdc]
    >   mntpath  [/data1-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdc]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               0b0da215-d518-4273-8994-3aeca6335e6d
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdc
    >   
    >   Creating mount point [/data1-01]
    >   Registering filesystem [/data1-01]
    >   UUID=0b0da215-d518-4273-8994-3aeca6335e6d /data1-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data1-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdc        512G   17M  510G   1% /data1-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Edwalafia][/dev/vdc][/data1-01]
    >   Wed 31 Jul 06:05:01 BST 2019
    >   Edwalafia
    >   [/dev/vdc][/data1-01]
    >   ---- ----
    >   hostname [Edwalafia]
    >   devpath  [/dev/vdc]
    >   mntpath  [/data1-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdc]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               3ff65a64-a991-412a-9f89-f25e3878f644
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdc
    >   
    >   Creating mount point [/data1-01]
    >   Registering filesystem [/data1-01]
    >   UUID=3ff65a64-a991-412a-9f89-f25e3878f644 /data1-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data1-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdc        512G   17M  510G   1% /data1-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Onoza][/dev/vdc][/data1-01]
    >   Wed 31 Jul 06:05:02 BST 2019
    >   Onoza
    >   [/dev/vdc][/data1-01]
    >   ---- ----
    >   hostname [Onoza]
    >   devpath  [/dev/vdc]
    >   mntpath  [/data1-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdc]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               63df5388-9b70-4fe6-8135-b5f30e156c00
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdc
    >   
    >   Creating mount point [/data1-01]
    >   Registering filesystem [/data1-01]
    >   UUID=63df5388-9b70-4fe6-8135-b5f30e156c00 /data1-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data1-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdc        512G   17M  510G   1% /data1-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Stedigo][/dev/vdd][/data2-01]
    >   Wed 31 Jul 06:05:04 BST 2019
    >   Stedigo
    >   [/dev/vdd][/data2-01]
    >   ---- ----
    >   hostname [Stedigo]
    >   devpath  [/dev/vdd]
    >   mntpath  [/data2-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdd]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               c6a0895d-0492-42f4-83fc-493abf929cb8
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdd
    >   
    >   Creating mount point [/data2-01]
    >   Registering filesystem [/data2-01]
    >   UUID=c6a0895d-0492-42f4-83fc-493abf929cb8 /data2-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data2-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdd        512G   17M  510G   1% /data2-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Angece][/dev/vdd][/data2-01]
    >   Wed 31 Jul 06:05:05 BST 2019
    >   Angece
    >   [/dev/vdd][/data2-01]
    >   ---- ----
    >   hostname [Angece]
    >   devpath  [/dev/vdd]
    >   mntpath  [/data2-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdd]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               92084190-b5eb-41f9-b445-e5cc79390e89
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdd
    >   
    >   Creating mount point [/data2-01]
    >   Registering filesystem [/data2-01]
    >   UUID=92084190-b5eb-41f9-b445-e5cc79390e89 /data2-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data2-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdd        512G   17M  510G   1% /data2-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Edwalafia][/dev/vdd][/data2-01]
    >   Wed 31 Jul 06:05:06 BST 2019
    >   Edwalafia
    >   [/dev/vdd][/data2-01]
    >   ---- ----
    >   hostname [Edwalafia]
    >   devpath  [/dev/vdd]
    >   mntpath  [/data2-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdd]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               2ed94ea1-58e3-4d54-b162-754310aff4f1
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdd
    >   
    >   Creating mount point [/data2-01]
    >   Registering filesystem [/data2-01]
    >   UUID=2ed94ea1-58e3-4d54-b162-754310aff4f1 /data2-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data2-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdd        512G   17M  510G   1% /data2-01
    >   
    >   ---- ---- ---- ----
    >   Mounting [Onoza][/dev/vdd][/data2-01]
    >   Wed 31 Jul 06:05:07 BST 2019
    >   Onoza
    >   [/dev/vdd][/data2-01]
    >   ---- ----
    >   hostname [Onoza]
    >   devpath  [/dev/vdd]
    >   mntpath  [/data2-01]
    >   ---- ----
    >   Creating btrfs filesystem [/dev/vdd]
    >   btrfs-progs v5.1
    >   See http://btrfs.wiki.kernel.org for more information.
    >   
    >   Label:              (null)
    >   UUID:               cddbe0b4-c0e0-40e0-8a02-9f4dcd2be528
    >   Node size:          16384
    >   Sector size:        4096
    >   Filesystem size:    512.00GiB
    >   Block group profiles:
    >     Data:             single            8.00MiB
    >     Metadata:         DUP               1.00GiB
    >     System:           DUP               8.00MiB
    >   SSD detected:       no
    >   Incompat features:  extref, skinny-metadata
    >   Number of devices:  1
    >   Devices:
    >      ID        SIZE  PATH
    >       1   512.00GiB  /dev/vdd
    >   
    >   Creating mount point [/data2-01]
    >   Registering filesystem [/data2-01]
    >   UUID=cddbe0b4-c0e0-40e0-8a02-9f4dcd2be528 /data2-01    btrfs    defaults,noatime    0  0
    >   Checking data space [/data2-01]
    >   Filesystem      Size  Used Avail Use% Mounted on
    >   /dev/vdd        512G   17M  510G   1% /data2-01


# -----------------------------------------------------
# Create our template compose file.
#[user@trop03]

cat > /tmp/kafka.yml << 'EOYML'

version: "3.2"

services:

    emily:
        image:
            "confluentinc/cp-kafka:4.1.1"
        restart:
            "no"
        ports:
            - "9092:9092"
            - "9093:9093"
        extra_hosts:
            - "${KAFKA_HOSTNAME}:127.0.0.2"
        environment:
            - "KAFKA_LISTENERS=jasminum://0.0.0.0:9092"
            - "KAFKA_ADVERTISED_LISTENERS=jasminum://${KAFKA_HOSTNAME}:9092"
            - "KAFKA_INTER_BROKER_LISTENER_NAME=jasminum"
            - "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=jasminum:PLAINTEXT"
            - "KAFKA_LOG_DIRS=${KAFKA_LOG_DIRS}"
            - "KAFKA_BROKER_ID=${KAFKA_BROKER_ID}"
            - "KAFKA_BROKER_RACK=${KAFKA_BROKER_RACK}"
            - "KAFKA_ZOOKEEPER_CONNECT=${KAFKA_ZOOKEEPER_CONNECT}"
            - "KAFKA_NUM_PARTITIONS=16"
            - "KAFKA_DEFAULT_REPLICATION_FACTOR=3"
            - "KAFKA_LOG_RETENTION_MS=-1"
            - "KAFKA_LOG_RETENTION_BYTES=-1"
            - "KAFKA_AUTO_CREATE_TOPICS_ENABLE=true"
            - "KAFKA_MESSAGE_MAX_BYTES=10485760"
        volumes:
EOYML


# -----------------------------------------------------
# Create a compose file for each node.
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            cp /tmp/kafka.yml /tmp/${vmname}-kafka.yml
        done


# -----------------------------------------------------
# Add the list of volumes to our compose files.
#[user@trop03]

        while read -r -u 5 vmname volpool volnum volsize voldev volmnt
            do
                if [[ ("${vmname}" != '') && ("${vmname:0:1}" != '#') ]]
                    then
                        if [[ ("${volmnt}" != '-') ]]
                            then
cat >> /tmp/${vmname}-kafka.yml << EOF
            - type:   "bind"
              source: "${volmnt}"
              target: "${volmnt}"
EOF
                            fi
                    fi
            done 5< "${HOME}/nodevols"


# -----------------------------------------------------
# Deploy our compose files.
#[user@trop03]

    source "${HOME}/ssh-options"

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname}]"
            scp \
                ${scpopts[*]} \
                /tmp/${vmname:?}-kafka.yml \
                ${sshuser:?}@${vmname:?}:kafka.yml
        done

    >   ---- ----
    >   Node [Stedigo]
    >   Stedigo-kafka.yml       100% 1259     1.2MB/s   00:00
    >   ---- ----
    >   Node [Angece]
    >   Angece-kafka.yml        100% 1259     1.3MB/s   00:00
    >   ---- ----
    >   Node [Edwalafia]
    >   Edwalafia-kafka.yml     100% 1259     1.2MB/s   00:00
    >   ---- ----
    >   Node [Onoza]
    >   Onoza-kafka.yml         100% 1259     1.2MB/s   00:00


# -----------------------------------------------------
# Make a list of our Zookeeper nodes.
#[user@trop03]

    zklist=${zknames[*]}
    zklist=${zklist// /,}

    echo "zklist [${zklist}]"

    >   zklist [Fosauri,Marpus,Byflame]


# -----------------------------------------------------
# Deploy a compose ENV file to each node.
#[user@trop03]

    unset logdirs
    declare logdirs=(
        '/data1-01'
        '/data2-01'
        '/data1-01'
        '/data2-01'
        )

    for (( i=0 ; i < ${#kfnames[@]} ; i++ ))
        do
            vmname=${kfnames[$i]:?}
            logdir=${logdirs[$i]:?}

            echo "---- ----"
            echo "Node [${i:?}][${vmname:?}]"

            ssh \
                ${scpopts[*]} \
                ${sshuser:?}@${vmname:?} \
                "
                hostname
                date

cat > kafka.env << EOF
KAFKA_LOG_DIRS=${logdir:?}
KAFKA_BROKER_ID=$(($i+1))
KAFKA_BROKER_RACK=$(($i+1))
KAFKA_ZOOKEEPER_CONNECT=${zklist:?}
KAFKA_HOSTNAME=${vmname:?}
EOF

                ln -sf kafka.env .env
                "
        done

    >   ---- ----
    >   Node [0][Stedigo]
    >   Stedigo
    >   Wed 31 Jul 06:08:46 BST 2019
    >   ---- ----
    >   Node [1][Angece]
    >   Angece
    >   Wed 31 Jul 06:08:47 BST 2019
    >   ---- ----
    >   Node [2][Edwalafia]
    >   Edwalafia
    >   Wed 31 Jul 06:08:47 BST 2019
    >   ---- ----
    >   Node [3][Onoza]
    >   Onoza
    >   Wed 31 Jul 06:08:48 BST 2019


# -----------------------------------------------------
# Check the compose ENV file on each node.
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname}]"
            ssh \
                ${sshopts[*]} \
                ${sshuser:?}@${vmname:?} \
                    "
                    hostname
                    date
                    cat .env
                    "
        done

    >   ---- ----
    >   Node [Stedigo]
    >   Stedigo
    >   Wed 31 Jul 06:09:07 BST 2019
    >   KAFKA_LOG_DIRS=/data1-01
    >   KAFKA_BROKER_ID=1
    >   KAFKA_BROKER_RACK=1
    >   KAFKA_ZOOKEEPER_CONNECT=Fosauri,Marpus,Byflame
    >   KAFKA_HOSTNAME=Stedigo
    >   ---- ----
    >   Node [Angece]
    >   Angece
    >   Wed 31 Jul 06:09:08 BST 2019
    >   KAFKA_LOG_DIRS=/data2-01
    >   KAFKA_BROKER_ID=2
    >   KAFKA_BROKER_RACK=2
    >   KAFKA_ZOOKEEPER_CONNECT=Fosauri,Marpus,Byflame
    >   KAFKA_HOSTNAME=Angece
    >   ---- ----
    >   Node [Edwalafia]
    >   Edwalafia
    >   Wed 31 Jul 06:09:08 BST 2019
    >   KAFKA_LOG_DIRS=/data1-01
    >   KAFKA_BROKER_ID=3
    >   KAFKA_BROKER_RACK=3
    >   KAFKA_ZOOKEEPER_CONNECT=Fosauri,Marpus,Byflame
    >   KAFKA_HOSTNAME=Edwalafia
    >   ---- ----
    >   Node [Onoza]
    >   Onoza
    >   Wed 31 Jul 06:09:09 BST 2019
    >   KAFKA_LOG_DIRS=/data2-01
    >   KAFKA_BROKER_ID=4
    >   KAFKA_BROKER_RACK=4
    >   KAFKA_ZOOKEEPER_CONNECT=Fosauri,Marpus,Byflame
    >   KAFKA_HOSTNAME=Onoza


# -----------------------------------------------------
# Start Kafka on each node.
#[user@trop03]

    for vmname in ${kfnames[@]}
        do
            echo "---- ----"
            echo "Node [${vmname}]"
            ssh \
                ${scpopts[*]} \
                ${sshuser:?}@${vmname:?} \
                "
                hostname
                date
                docker-compose \
                    --file kafka.yml \
                    up -d
                "
        done

    >   ---- ----
    >   Node [Stedigo]
    >   Stedigo
    >   Wed 31 Jul 06:09:26 BST 2019
    >   Creating network "stevedore_default" with the default driver
    >   Pulling emily (confluentinc/cp-kafka:4.1.1)...
    >   4.1.1: Pulling from confluentinc/cp-kafka
    >   Digest: sha256:4f5d6f4ba368fc05c0d0f1390bf107ae45fab24ed4bd7aac75aed3086ae5ddab
    >   Status: Downloaded newer image for confluentinc/cp-kafka:4.1.1
    >   Creating stevedore_emily_1 ... done
    >   ---- ----
    >   Node [Angece]
    >   Angece
    >   Wed 31 Jul 06:09:48 BST 2019
    >   Creating network "stevedore_default" with the default driver
    >   Pulling emily (confluentinc/cp-kafka:4.1.1)...
    >   4.1.1: Pulling from confluentinc/cp-kafka
    >   Digest: sha256:4f5d6f4ba368fc05c0d0f1390bf107ae45fab24ed4bd7aac75aed3086ae5ddab
    >   Status: Downloaded newer image for confluentinc/cp-kafka:4.1.1
    >   Creating stevedore_emily_1 ... done
    >   ---- ----
    >   Node [Edwalafia]
    >   Edwalafia
    >   Wed 31 Jul 06:10:10 BST 2019
    >   Creating network "stevedore_default" with the default driver
    >   Pulling emily (confluentinc/cp-kafka:4.1.1)...
    >   4.1.1: Pulling from confluentinc/cp-kafka
    >   Digest: sha256:4f5d6f4ba368fc05c0d0f1390bf107ae45fab24ed4bd7aac75aed3086ae5ddab
    >   Status: Downloaded newer image for confluentinc/cp-kafka:4.1.1
    >   Creating stevedore_emily_1 ... done
    >   ---- ----
    >   Node [Onoza]
    >   Onoza
    >   Wed 31 Jul 06:10:32 BST 2019
    >   Creating network "stevedore_default" with the default driver
    >   Pulling emily (confluentinc/cp-kafka:4.1.1)...
    >   4.1.1: Pulling from confluentinc/cp-kafka
    >   Digest: sha256:4f5d6f4ba368fc05c0d0f1390bf107ae45fab24ed4bd7aac75aed3086ae5ddab
    >   Status: Downloaded newer image for confluentinc/cp-kafka:4.1.1
    >   Creating stevedore_emily_1 ... done


# -----------------------------------------------------
# -----------------------------------------------------
# Update the ssh keys for each node.
#[user@desktop]

    source "${HOME}/libvirt.settings"
    source "${HOME}/ssh-options"

    for vmname in ${kfnames[@]}
        do
            ssh-keygen \
                -q -R \
                    "${vmname:?}"

        done

    >   /home/Zarquan/.ssh/known_hosts updated.
    >   Original contents retained as /home/Zarquan/.ssh/known_hosts.old
    >   /home/Zarquan/.ssh/known_hosts updated.
    >   Original contents retained as /home/Zarquan/.ssh/known_hosts.old
    >   /home/Zarquan/.ssh/known_hosts updated.
    >   Original contents retained as /home/Zarquan/.ssh/known_hosts.old
    >   /home/Zarquan/.ssh/known_hosts updated.
    >   Original contents retained as /home/Zarquan/.ssh/known_hosts.old


# -----------------------------------------------------
# Check we can login to each node.
#[user@desktop]

    source "${HOME}/libvirt.settings"
    source "${HOME}/ssh-options"

    for vmname in ${kfnames[@]}
        do
            ssh \
                ${sshopts[*]} \
                ${sshuser:?}@${vmname:?} \
                    "
                    date
                    hostname
                    "
        done

    >   The authenticity of host 'stedigo (<no hostip for proxy command>)' can't be established.
    >   ECDSA key fingerprint is SHA256:Cg75JJWFH+YY08N5rd6ulZtCQqi+4Yiylzs6Mltzg2w.
    >   Are you sure you want to continue connecting (yes/no)? yes
    >   Wed 31 Jul 06:15:37 BST 2019
    >   Stedigo
    >   The authenticity of host 'angece (<no hostip for proxy command>)' can't be established.
    >   ECDSA key fingerprint is SHA256:v2fYESMHCAo35nwXYXQR70UqFkIV8aGyKcjZYQtHGOc.
    >   Are you sure you want to continue connecting (yes/no)? yes
    >   Wed 31 Jul 06:15:40 BST 2019
    >   Angece
    >   The authenticity of host 'edwalafia (<no hostip for proxy command>)' can't be established.
    >   ECDSA key fingerprint is SHA256:LVgrJ2g4P6n6NFuIPgB/8Kba13GNVK4j/pA1s0GCAow.
    >   Are you sure you want to continue connecting (yes/no)? yes
    >   Wed 31 Jul 06:15:43 BST 2019
    >   Edwalafia
    >   The authenticity of host 'onoza (<no hostip for proxy command>)' can't be established.
    >   ECDSA key fingerprint is SHA256:RY/bzhDNy1RSmOnW/L2BxIMU476WH5N9IQnTr3dM6Fg.
    >   Are you sure you want to continue connecting (yes/no)? yes
    >   Wed 31 Jul 06:15:46 BST 2019
    >   Onoza


# -----------------------------------------------------
# Tail the logs on each node.
# https://www.systutorials.com/docs/linux/man/1-gnome-terminal/
# https://www.systutorials.com/docs/linux/man/7-X/#lbAH
#[user@desktop]

    mate-terminal \
        --geometry '160x10+25+25' \
        --command '
            ssh -t Stedigo "
                date
                hostname
                docker logs -f stevedore_emily_1
                \${SHELL}
                "
            '

    >   [2019-07-31 05:09:55,196] INFO starting (kafka.server.KafkaServer)
    >   [2019-07-31 05:09:55,197] INFO Connecting to zookeeper on Fosauri,Marpus,Byflame (kafka.server.KafkaServer)
    >   [2019-07-31 05:09:55,216] INFO [ZooKeeperClient] Initializing a new session to Fosauri,Marpus,Byflame. (kafka.zookeeper.ZooKeeperClient)
    >   [2019-07-31 05:09:55,222] INFO Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:09:55,222] INFO Client environment:host.name=fe7aece71406 (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:09:55,222] INFO Client environment:java.version=1.8.0_172 (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:09:55,222] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:09:55,222] INFO Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.ZooKeeper)


    sleep 1
    mate-terminal \
        --geometry '160x10+125+125' \
        --command '
            ssh -t Angece "
                date
                hostname
                docker logs -f stevedore_emily_1
                \${SHELL}
                "
            '

    >   [2019-07-31 05:10:16,926] INFO starting (kafka.server.KafkaServer)
    >   [2019-07-31 05:10:16,927] INFO Connecting to zookeeper on Fosauri,Marpus,Byflame (kafka.server.KafkaServer)
    >   [2019-07-31 05:10:16,948] INFO [ZooKeeperClient] Initializing a new session to Fosauri,Marpus,Byflame. (kafka.zookeeper.ZooKeeperClient)
    >   [2019-07-31 05:10:16,955] INFO Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:10:16,955] INFO Client environment:host.name=475500f3e6b1 (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:10:16,955] INFO Client environment:java.version=1.8.0_172 (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:10:16,955] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:10:16,955] INFO Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:10:16,956] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b32.jar:/usr--END--
    >   
    >   
    >       sleep 1
    >       mate-terminal \
    >           --geometry '160x10+225+225' \
    >           --command '
    >               ssh -t Edwalafia "
    >                   date
    >                   hostname
    >                   docker logs -f stevedore_emily_1
    >                   \${SHELL}
    >                   "
    >               '
    >   
    >   [2019-07-31 05:10:39,349] INFO starting (kafka.server.KafkaServer)
    >   [2019-07-31 05:10:39,350] INFO Connecting to zookeeper on Fosauri,Marpus,Byflame (kafka.server.KafkaServer)
    >   [2019-07-31 05:10:39,369] INFO [ZooKeeperClient] Initializing a new session to Fosauri,Marpus,Byflame. (kafka.zookeeper.ZooKeeperClient)
    >   [2019-07-31 05:10:39,376] INFO Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:10:39,376] INFO Client environment:host.name=dcc99469f849 (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:10:39,376] INFO Client environment:java.version=1.8.0_172 (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:10:39,376] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:10:39,376] INFO Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.ZooKeeper)


    sleep 1
    mate-terminal \
        --geometry '160x10+325+325' \
        --command '
            ssh -t Onoza "
                date
                hostname
                docker logs -f stevedore_emily_1
                \${SHELL}
                "
            '

    >   [2019-07-31 05:11:01,286] INFO starting (kafka.server.KafkaServer)
    >   [2019-07-31 05:11:01,287] INFO Connecting to zookeeper on Fosauri,Marpus,Byflame (kafka.server.KafkaServer)
    >   [2019-07-31 05:11:01,306] INFO [ZooKeeperClient] Initializing a new session to Fosauri,Marpus,Byflame. (kafka.zookeeper.ZooKeeperClient)
    >   [2019-07-31 05:11:01,311] INFO Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:11:01,311] INFO Client environment:host.name=02f43b0cb160 (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:11:01,311] INFO Client environment:java.version=1.8.0_172 (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:11:01,311] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
    >   [2019-07-31 05:11:01,311] INFO Client environment:java.home=/usr/lib/jvm/zulu-8-amd64/jre (org.apache.zookeeper.ZooKeeper)










